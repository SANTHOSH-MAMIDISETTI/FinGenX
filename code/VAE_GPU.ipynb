{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0e913f8",
   "metadata": {},
   "source": [
    "# VAE Latent Dimension Evaluation on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a620b6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6010c771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load the original data\n",
    "data = pd.read_excel('/kaggle/input/dataaa/default of credit card clients.xls')\n",
    "\n",
    "# Drop the first row and reset the index if needed\n",
    "data = data.drop(index=0)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Convert columns to numeric\n",
    "data = data.apply(pd.to_numeric, errors='coerce').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566b23bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_periodic_weighted_loss(original, generated):\n",
    "    sine_weights = torch.sin(original * np.pi)\n",
    "    weighted_diff = torch.sum(sine_weights * (original - generated).pow(2))\n",
    "    return weighted_diff.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a7d2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_latent_dimensions(data, latent_dims, input_dim, epochs=50, batch_size=64, learning_rate=0.001):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    results = {}\n",
    "\n",
    "    for latent_dim in latent_dims:\n",
    "        class Encoder(nn.Module):\n",
    "            def __init__(self, input_dim, latent_dim):\n",
    "                super(Encoder, self).__init__()\n",
    "                self.fc1 = nn.Linear(input_dim, 256)\n",
    "                self.fc2 = nn.Linear(256, 128)\n",
    "                self.fc3 = nn.Linear(128, 64)\n",
    "                self.batch_norm1 = nn.BatchNorm1d(256)\n",
    "                self.batch_norm2 = nn.BatchNorm1d(128)\n",
    "                self.dropout = nn.Dropout(0.3)\n",
    "                self.z_mean = nn.Linear(64, latent_dim)\n",
    "                self.z_log_var = nn.Linear(64, latent_dim)\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = F.relu(self.batch_norm1(self.fc1(x)))\n",
    "                x = F.relu(self.batch_norm2(self.fc2(x)))\n",
    "                x = self.dropout(F.relu(self.fc3(x)))\n",
    "                z_mean = self.z_mean(x)\n",
    "                z_log_var = self.z_log_var(x)\n",
    "                return z_mean, z_log_var\n",
    "\n",
    "        class Decoder(nn.Module):\n",
    "            def __init__(self, latent_dim, input_dim):\n",
    "                super(Decoder, self).__init__()\n",
    "                self.fc1 = nn.Linear(latent_dim, 64)\n",
    "                self.fc2 = nn.Linear(64, 128)\n",
    "                self.fc3 = nn.Linear(128, 256)\n",
    "                self.batch_norm1 = nn.BatchNorm1d(64)\n",
    "                self.batch_norm2 = nn.BatchNorm1d(128)\n",
    "                self.dropout = nn.Dropout(0.3)\n",
    "                self.fc4 = nn.Linear(256, input_dim)\n",
    "\n",
    "            def forward(self, z):\n",
    "                x = F.relu(self.batch_norm1(self.fc1(z)))\n",
    "                x = F.relu(self.batch_norm2(self.fc2(x)))\n",
    "                x = self.dropout(F.relu(self.fc3(x)))\n",
    "                x = torch.sigmoid(self.fc4(x))\n",
    "                return x\n",
    "\n",
    "        class VAE(nn.Module):\n",
    "            def __init__(self, input_dim, latent_dim):\n",
    "                super(VAE, self).__init__()\n",
    "                self.encoder = Encoder(input_dim, latent_dim)\n",
    "                self.decoder = Decoder(latent_dim, input_dim)\n",
    "\n",
    "            def forward(self, x):\n",
    "                z_mean, z_log_var = self.encoder(x)\n",
    "                z = reparameterize(z_mean, z_log_var)\n",
    "                reconstruction = self.decoder(z)\n",
    "                return reconstruction, z_mean, z_log_var\n",
    "\n",
    "        def reparameterize(z_mean, z_log_var):\n",
    "            std = torch.exp(0.5 * z_log_var)\n",
    "            epsilon = torch.randn_like(std)\n",
    "            return z_mean + std * epsilon\n",
    "\n",
    "        def periodic_weighted_loss(original, reconstruction, z_mean, z_log_var, input_dim):\n",
    "            reconstruction_loss = F.mse_loss(reconstruction, original, reduction='sum')\n",
    "            kl_loss = -0.5 * torch.sum(1 + z_log_var - z_mean.pow(2) - torch.exp(z_log_var))\n",
    "            sine_weights = torch.sin(original * np.pi)\n",
    "            weighted_diff = torch.sum(sine_weights * (original - reconstruction).pow(2))\n",
    "            total_loss = (reconstruction_loss + kl_loss + weighted_diff) / input_dim\n",
    "            return total_loss\n",
    "\n",
    "        normalized_data = (data - data.mean()) / data.std()\n",
    "        normalized_data = torch.tensor(normalized_data.values, dtype=torch.float32).to(device)\n",
    "        dataset = TensorDataset(normalized_data)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        vae = VAE(input_dim, latent_dim).to(device)\n",
    "        optimizer = torch.optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "        vae.train()\n",
    "        total_loss = 0\n",
    "        for epoch in range(epochs):\n",
    "            for batch in dataloader:\n",
    "                x = batch[0].to(device)\n",
    "                optimizer.zero_grad()\n",
    "                reconstruction, z_mean, z_log_var = vae(x)\n",
    "                loss = periodic_weighted_loss(x, reconstruction, z_mean, z_log_var, input_dim)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "        results[latent_dim] = total_loss / len(dataset)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa69f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dims = [2, 4, 6, 8, 10, 12, 15]\n",
    "input_dim = data.shape[1]\n",
    "epochs = 50\n",
    "results = evaluate_latent_dimensions(data, latent_dims, input_dim, epochs=epochs)\n",
    "\n",
    "for latent_dim, loss in results.items():\n",
    "    print(f\"Latent Dimension: {latent_dim}, Loss: {loss:.4f}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
